---
title: "Arbeidskrav2"
author: "Håvard Crantz Lorentzen, Hermann Moen, Margit Dahl Sørensen, Jacob Mollatt"
date: "28 9 2021"
output:
  bibliography: referanser.bib
  word_document: default
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE, warning=FALSE, echo=TRUE}
library(tidyverse)
library(readxl)
library(ggplot2)
library(latexpdf)
library(knitr)


lactate <- read_excel("data til laktatprofil.xlsx", na="NA")


lactate %>%
  # Select columns needed for analysis
  select(FP, time, lac.75:lac.300) %>%
  # Only one participant and time-point
  filter(time == "pre", FP == 6) %>%
  # Pivot to long format data using the lactate columns
  pivot_longer(names_to = "watt", 
               values_to = "lactate", 
               names_prefix = "lac.",
               names_transform = list(watt = as.numeric),
               cols = lac.75:lac.300) %>%
   # Remove NA (missing) values to avoid warning/error messages.
  filter(!is.na(lactate))%>%
  # Plot the data, group = subject needed to connect the points
  ggplot(aes(watt, lactate, group = FP))  + 
  geom_line(lty = 2) +
  geom_point(shape = 21, fill = "lightblue", size = 2.5) +
# Adding straight lines at specific values
  geom_hline(yintercept = 4, color = "red") +
  geom_vline(xintercept = 341.5, color = "blue") + # Adding a straight line from a linear model
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x, color = "#e41a1c") +

 # Adding a polynomial linear model to the plot
  
  # poly(x, 2) add a second degree polynomial model.
  geom_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, 2), color = "#377eb8") +
  # poly(x, 3) add a third degree polynomial model.
  geom_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, 3), color = "#4daf4a") +
  # poly(x, 4) add a forth degree polynomial model.
  geom_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, 4), color = "#ff7f00") 

```


```{r}
library(tidyverse)
library(readxl)
library(ggplot2)

lactate <- read_excel("data til laktatprofil.xlsx", na="NA")

lactate1 <- lactate %>%
  # Select columns needed for analysis
  select(FP, time, lac.75:lac.300) %>%
  # Only one participant and time-point
  filter(time == "pre", FP == 6) %>%
  # Pivot to long format data using the lactate columns
  pivot_longer(names_to = "watt", 
               values_to = "lactate", 
               names_prefix = "lac.",
               names_transform = list(watt = as.numeric),
               cols = lac.75:lac.300) %>%
   # Remove NA (missing) values to avoid warning/error messages.
  filter(!is.na(lactate))

# fit "straight line" model
m1 <- lm(lactate ~ watt, data = lactate1)

# fit second degree polynomial
m2 <- lm(lactate ~ poly(watt, 2, raw = TRUE), data = lactate1)

# fit third degree polynomial
m3 <- lm(lactate ~ poly(watt, 3, raw = TRUE), data = lactate1)

# fit forth degree polynomial
m4 <- lm(lactate ~ poly(watt, 4, raw = TRUE), data = lactate1)

# Store all residuals as new variables
lactate1$resid.m1 <- resid(m1)
lactate1$resid.m2 <- resid(m2)
lactate1$resid.m3 <- resid(m3)
lactate1$resid.m4 <- resid(m4)

lactate1 %>%
  # gather all the data from the models
  pivot_longer(names_to = "model", 
               values_to = "residual", 
               names_prefix = "resid.", 
               names_transform = list(residual = as.numeric), 
               cols = resid.m1:resid.m4) %>%
  # Plot values with the observed watt on x axis and residual values at the y
  ggplot(aes(watt, residual, fill = model)) + geom_point(shape = 21, size = 3) +
  
  # To set the same colors/fills as above we use scale fill manual
  scale_fill_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#ff7f00"))


#new data data frame
ndf <- data.frame(watt = seq(from = 125, to = 275, by = 0.1)) # high resolution, we can find the nearest10:th a watt

ndf$predictions <- predict(m4, newdata = ndf)

lactate_threshold4 <- ndf %>%
  filter(abs(predictions - 4) == min(abs(predictions - 4)))
 
lactate_threshold2 <- ndf %>%
  filter(abs(predictions - 2) == min(abs(predictions - 2)))
```

```{r}

lactate <- read_excel("data til laktatprofil.xlsx", na="NA")

lactate2 <- lactate %>%
  select(FP, time, lac.75:lac.300) %>%
  pivot_longer(names_to = "watt", 
               values_to = "lactate", 
               names_prefix = "lac.", 
              names_transform = list(watt = as.numeric), 
              cols = lac.75:lac.300) %>%
  filter(FP == 6, 
         time == "pre", 
         !is.na(lactate)) %>% # Remove NA values
  print()

# Fit the model 
model <- lm(lactate ~ watt + I(watt^2) + I(watt^3), data = lactate2)


# Predict lactate values over all observed watt values
# calculate the smallest distance from the fixed lactate value 

new_data <- data.frame(watt = seq(from = min(lactate2$watt), to = max(lactate2$watt), by = 0.1))

new_data$dist <- abs(predict(model, newdata = new_data) - 4)

# Find the smallest value of predicted - fixed lacate value
new_data %>%
  filter(dist == min(dist)) # Where the dist value equals the minimum dist value
lt <- function(data) {
  
  # Fit a 3 degree polynomial model
  m <- lm(lactate ~ watt + I(watt^2) + I(watt^3), data = data)
  
  # Store a data frame with exercise intensities
  new_data <- data.frame(watt = seq(from = min(data$watt), to = max(data$watt), by = 0.01))
  
  # Predict using the new data, predicting lactate values at each 
  new_data$pred <- predict(m, newdata = new_data)
  
  # calculate deviation from the lactate value of interest
  new_data$watt.4mmol <- abs(new_data$pred - 4)
  new_data$watt.2mmol <- abs(new_data$pred - 2)

  # Create a results data frame
  results <- data.frame(watt.4mmol = new_data[which.min(new_data$watt.4mmol),1], watt.2mmol = new_data[which.min(new_data$watt.2mmol),1])

  # Return the data frame
  return(results)
  
}


lactate3 <- lactate %>%
  select(FP, time, lac.75:lac.300) %>%
  pivot_longer(names_to = "watt", 
               values_to = "lactate", 
               names_prefix = "lac.", 
              names_transform = list(watt = as.numeric), 
              cols = lac.75:lac.300) %>%
  filter(!is.na(lactate)) %>% # Remove NA values
  group_by(time, FP) %>%
   mutate(n = n()) %>%
  filter(n >= 4) %>%
  # Use grouup modify to apply the function to all participants per time-point (and group)
  group_modify(~ lt(.)) %>%
  print()

```

```{r}
lactate4 <- lactate3 %>%
  group_by(FP) %>%
  mutate(n = n()) %>%
  filter(n == 2) %>%
pivot_wider(names_from = time, values_from = c(watt.4mmol, watt.2mmol)) %>%
    print()

    
TE4 <- lactate4 %>%
  mutate(diff = watt.4mmol_post - watt.4mmol_pre) %>% 
  ungroup() %>%
  # Change/difference score
  summarise(s = sd(diff, na.rm = TRUE), # Summarize to calculate sd, and... 
            m = mean(c(watt.4mmol_post, watt.4mmol_pre)), # mean
            te = s / sqrt(2), 1, # the typical error.
            cv = 100 * (te / m), 1, 
            L = qt(0.975, 4) * s) %>%  # Calculate as a percentage of the mean
  print()

TE2 <- lactate4 %>%
  mutate(diff = watt.2mmol_post - watt.2mmol_pre) %>% 
   ungroup() %>%# Change/difference score
  summarise(s = sd(diff),  # Summarize to calculate sd, and... 
            m = mean(c(watt.2mmol_post, watt.2mmol_pre)), # mean
            te = s / sqrt(2), 1, # the typical error.
            cv = 100 * (te / m), 1, 
            L = qt(0.975, 4) * s) %>% # Calculate as a percentage of the mean

print()

  cv4 <- round(TE4$cv, 2)
  cv2 <- round(TE2$cv, 2)
  
```


```{r}

ladder <- data.frame(dist = c(1.0,25.5,63.0,111.0,165.5,241.0,332.5,460.5,540.5,637.5), 
                     
                     mw = c(1000, 900, 800, 
                            700, 600, 500,
                            400, 300, 250, 
                            200))
            

# Create a new data frame of unknowns
unknown <- data.frame(dist = c(95.5,303.8,413.8,95.5,300.0,409.5))


 #Fit the model
cal <- lm(log(mw) ~ dist, data = ladder)

# Check model performance, R^2 should be ~ 1.
summary(cal)

preds <- exp(predict(cal, newdata = unknown))

preds

round(preds,2)
```


```{r}
library(exscidata); library(tidyverse); library(dplyr)
data("hypertrophy")
glimpse(hypertrophy)


Testo_DXA <- hypertrophy %>%
  select(PARTICIPANT, DXA_LBM_T1:DXA_LBM_T3, TESTOSTERONE_T1:TESTOSTERONE_T3) %>%
  rowwise() %>%
  mutate(testoavg. = mean(c(TESTOSTERONE_T1, TESTOSTERONE_T2, TESTOSTERONE_T3), na.rm=TRUE)) %>%

  ungroup() %>%
  mutate(DXA_change = DXA_LBM_T3 - DXA_LBM_T1) %>%
  group_by() %>%
  select(PARTICIPANT, testoavg., DXA_change) %>%
  filter(! is.nan(testoavg.)) %>%
  
  print()
  
  
# Fit the model

m1 <- lm(DXA_change ~ testoavg., data = Testo_DXA) %>%

summary(m1) %>%
  print()


#Reg

library(broom) 

tidy(m1) %>%
  print()
 

#plotte figur

Testo_DXA %>%
  ggplot(aes(testoavg., DXA_change)) + geom_point(shape = 19) + theme_minimal() + geom_smooth(method = "lm") + labs(x = "Gjennomsnittlig testosteron", y = "Endring i DXA") 

print()

```

## Resultater 
Typefeil er ved 4mmol laktat er `r cv4`% og ved 2mmol laktat er den `r cv2`%
Predikerte størrelser på DNA-fragmentene er `r round(preds,2)`.

I datasettet hypertrophy finner vi flere forskjellige variabler tilknyttet muskelvekst. Blant annet finnes det en måling av deltakernes testosteronverdier ved tre tidspunkt fra intervensjonen, og mager kroppsmasse målt i DXA ved tre tidspunkter. Vi ville se på en eventuell korrelasjon mellom den gjennomsnittlige testosteronverdien  og endringen i mager kroppsmasse fra test 1 til 3. 

##Diskusjon

Testosteron er et potent anabolsk hormon, som stimulerer muskel-proteinsyntesen[@Florini; @Mauras], og forbedrer intramuskulært opptak av aminosyrer,[@Balieu] som sammen resulterer i en forbedring i total protein balanse[@Ferrando]. Basert på dette ser vi for oss at testosteron nivået, vil ha en positiv innvirkning på endringen i mager kroppsmasse.

For hver økning i testosteron vil endringen i mager kroppsmasse endres med -0,00434 +- 0,00167std.. Dette betyr i praksis at høyere testosteron ga mindre endring i mager kroppsmasse p=0,0157. P-verdien forteller oss hvor overraskende utfallet er hvis null hypotesen er sann. Altså vil det være et veldig overraskende funn hvis null hypotesen var sann, og vi kan kalle funnet statistisk signinfikant p</=0,05, når signifikans grensen er satt til 0,05 Std. er en forkortelse for standard avvik, og viser oss det gjennomsnittelige avviket fra gjennomsnittet i målingen. 

Vi så for oss en sammenheng mellom gjennomsnittlig testosteron verdi, og endringen i mager kroppsmasse. Vi tenkte korrelasjonen skulle være positiv, altså mere testosteron, ville resultere i større økning i mager masse. Dette er basert på litteratur nevnt over. Det viste seg at for hver verdi med testosteron ville den magre kroppsmassen endres med -0,00434, p=0,0157. 

Vi ønsker å vite om dette resultatet er noe vi kan stole på, og dermed kunne late som at funnet her er en sannhet. Vi må derfor kunne si noe om hvor reliabelt det er, og hvor sannsynlig det er at forskjellen i dataen skyldes sammenhengen mellom variablene og ikke støy. Standard feil er et estimat på reliabiliteten til dataene. Reliabiliteten kan forklares som hvor stor verdien til den tilfeldige variasjonen i et mål vil være fra gang til gang, når målet er gjort under like forhold [@Hopkins]. Standardfeilen beregnes ved hjelp av «within subject standard deviation». Dette datasettet ga oss en standardfeil i målingene på 0,00167. 

Nå vet vi hvor mye som kan forventes skyldes støy, og vi ønsker nå å vite om funnet vårt om korrelasjonen er noe vi kan late som er sant.  Til dette vil vi bruke p-verdi. P-verdien forteller oss hvor overraskende utfallet er hvis null hypotesen er sann. Altså vil det være et veldig overraskende funn hvis null hypotesen var sann, og vi kan kalle funnet statistisk signinfikant p</=0,05, når signifikans grensen er satt til 0,05. Det betyr at i denne situasjonen kan vi late som at funnet er sant. 

Altså vet vi nå at det er et veldig overraskende funn om null hypotesen er sann, som i praksis gjør at vi kan late som at funnet er sant, da den som nevnt var under grenseverdien for statistisk signifikans. Vi kan også se på om null hypotesen skal avises eller følges ved hjelp av t-verdi. T-verdien er enkelt sagt forholdet mellom verdien og standardfeilen. Jo høyere verdien er, jo sikrere kan vi være på at null hypotesen kan avvises. Dette gjøres i hypotesetesting i students t-test.  Et akseptabelt resultat i t-verdi er 2, eller -2. I dette tilfellet her var t-verdien 2,60, som gir oss enda en grunn til å late som at funnet vil være sant i alle tilfeller.  
